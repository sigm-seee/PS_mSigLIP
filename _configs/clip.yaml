training: true
dataset_root_dir: .
img_size: &size ${tuple:384, 128}
distributed: false

# Dataset
dataset:
  dataset_name: VN3K_EN

  text_length: &model_max_length 77
  batch_size: 16
  sampler: identity
  test_batch_size: 64
  num_instance: 1
  num_workers: 2

# Loss
loss:
  # Main contrastive loss
  NITC: true
  nitc_loss_weight: 1.0
  softlabel_ratio: 0.5

  MVS: true

  # Self-supervised losses following SimCLR
  SS: true
  simclr_temperature: 0.1
  ss_loss_weight: 0.4

  # Masked language modeling loss
  MLM: false
  mlm_loss_weight: 1.0
  cmt_depth: 4

  ID: false
  id_loss_weight: 1.0

  SDM: false
  sdm_loss_weight: 1.0

  RITC: true
  ritc_eps: 1.0e-2
  ritc_loss_weight: 1.0

  CITC: true
  citc_inmodal_weight: 0.25
  citc_intermodal_weight: 0.25
  citc_loss_weight: 0.1

# Augmentation
aug:
  image_random_k: 2
  text_random_k: -1
  img:
    size: *size
    augment_cfg:
      torchvision.transforms.ColorJitter:
        brightness: 0.1
        contrast: 0.1
        saturation: 0.1
        hue: 0
      torchvision.transforms.RandomRotation:
        degrees: 15
      torchvision.transforms.RandomResizedCrop:
        size: *size
        scale: ${tuple:0.9, 1.0}
        # ratio: [0.75, 1.3333333333333333]
        antialias: true
      torchvision.transforms.RandomGrayscale:
        p: 0.1
      torchvision.transforms.RandomHorizontalFlip:
        p: 0.5
      torchvision.transforms.RandomErasing:
        scale: ${tuple:0.10, 0.20}
  text:
    augment_cfg:
      datasets.augmentation.text_transform.random_deletion:
        p: 0.05

# Trainer
trainer:
  ckpt_path:
  default_root_dir: ./logs
  max_epochs: &max_epoch 5
  precision: bf16-mixed
  accumulate_grad_batches: 1
  accelerator: gpu
  gradient_clip_val: 1.0
  gradient_clip_algorithm: value
  log_every_n_steps: 1
  detect_anomaly: false
  enable_progress_bar: true
  enable_model_summary: true
  enable_checkpointing: true
  # fast_dev_run: 16
  num_sanity_val_steps: 0
  # check_val_every_n_epoch: 1
  # overfit_batches: 10

# Optimizer
optimizer:
  type: torch.optim.AdamW
  # type: bitsandbytes.optim.AdamW8bit
  lr: 1e-4
  lr_factor: 5.0
  bias_lr_factor: 2.0
  weight_decay: 0.02
  weight_decay_bias: 0.0
  betas: [0.9, 0.98]
  eps: 1.0e-8

# Scheduler
scheduler:
  type: solver.lr_scheduler.LRSchedulerWithWarmup
  mode: cosine
  warmup_epochs: 1
  warmup_method: linear
  total_epochs: *max_epoch
  start_lr: 1.0e-6
  end_lr: 5.0e-6

# scheduler:
#   type: transformers.optimization.get_cosine_schedule_with_warmup
#   num_warmup_steps: 538
#   num_training_steps: 2690

# Tokenizer
tokenizer:
  type: model.clip.tokenization_clip.CLIPTokenizer
  vocab_size: &vocab_size 49408
  pretrained_model_name_or_path: ./clip_checkpoints
  model_max_length: *model_max_length
  add_mask_token: false

# Backbone
backbone:
  type: model.clip.modeling_clip.CLIPModel
  path: clip_checkpoints/pytorch_model.bin
  config_type: model.clip.configuration_clip.CLIPConfig
  original_model_include_cls_token: true
  text_config:
    vocab_size: *vocab_size
    max_position_embeddings: *model_max_length
    attention_dropout: 0.05
  vision_config:
    image_size: *size
    patch_size: 16
  num_visual_horizontal_patches: 8
  num_visual_vertical_patches: 24
  embedding_dim: 512
